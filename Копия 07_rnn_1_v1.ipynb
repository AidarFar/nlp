{"cells":[{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1703019798402,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"zKMq7dp2W15Y","outputId":"5576ae02-743f-4b2b-a29a-1255610880d7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":37}],"source":["import nltk\n","import pandas as pd\n","import plotly.express as px\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import Dataset, DataLoader\n","\n","nltk.download('punkt')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3IeYLxT2y--","executionInfo":{"status":"ok","timestamp":1703019800268,"user_tz":-180,"elapsed":1638,"user":{"displayName":"Nudel","userId":"11250601100287929999"}},"outputId":"320e890b-308f-46c2-964c-b4af189a8fc6"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jm-QilGISxkt"},"source":["## 1. Классификация фамилий (RNN)\n","\n","Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"]},{"cell_type":"markdown","metadata":{"id":"YdPr92i6k-If"},"source":["1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий.\n"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"ir6UUkl6l4tp"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n","        self.hidden_size = hidden_size\n","        self.input_size = input_size\n","\n","    def forward(self, x, h=None):\n","        batch_size, seq_size, emb_size = x.shape\n","        # инициализация тензора скрытых состояний\n","        if h is None:\n","            h = torch.zeros(batch_size, self.hidden_size).to(device=x.device)\n","        # проход по каждому элементу последовательностей s в батче и обновление скрытого состояния\n","        out = torch.zeros(batch_size, seq_size, self.hidden_size)\n","        for t in range(seq_size):\n","            h = self.rnn_cell(x[:, t, :], h)\n","            out[:, t] = h\n","\n","        # вернуть тензор всех наблюдавшихся скрытых состояний размера (batch_size, seq_len, hidden_size) и тензор скрытых состояний в последний момент времени\n","        return out, h"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"cMn3GkKojwih","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a9f40b4-a8d9-40b9-a456-9a0cba9d5876"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 5, 10])\n","tensor([[[ 0.0846,  0.2770,  0.0811],\n","         [-0.0281,  0.3388, -0.2196],\n","         [ 0.1774,  0.2804,  0.1090]]])\n","torch.Size([5, 10])\n"]}],"source":["batch_size, seq_len, embedding_size, hidden_size = 5, 5, 7, 10\n","test = torch.rand(batch_size, seq_len, embedding_size)\n","rnn = RNN(embedding_size, hidden_size)\n","with torch.no_grad():\n","    t = rnn(test)\n","    print(t[0].shape, t[0][:1, :3, :3], t[1].shape, sep='\\n')"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"8r2GiCx7kCBM"},"outputs":[],"source":["train_surnames = pd.read_csv('/content/drive/MyDrive/datasets/surnames.csv')\n","class_encoder = LabelEncoder()\n","train_surnames['nationality'] = class_encoder.fit_transform(train_surnames['nationality'])\n","\n","surname = train_surnames['surname'].str.lower()\n","nat = train_surnames['nationality']\n","n_classes = nat.nunique()\n","\n","x_train, x_test, y_train, y_test = train_test_split(surname, nat, test_size=0.2, random_state=1)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"yBmt4JDcmroA","outputId":"e37e4e19-5d70-4d83-fa40-323d9ff9c886"},"outputs":[{"output_type":"stream","name":"stdout","text":["18\n"]}],"source":["print(n_classes)"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"qt5N16ZfmxuP"},"outputs":[],"source":["class Vocab:\n","    def __init__(self, data):\n","        tokens = set()\n","        self.max_seq_len = 0\n","        for item in data:\n","            tokens.update(item)\n","            self.max_seq_len = max(self.max_seq_len, len(item))\n","        self.unk_token = '<UNK>'\n","        self.pad_token = '<PAD>'\n","        self.idx_to_token = dict(enumerate(tokens, 2))\n","        self.idx_to_token[1] = self.unk_token\n","        self.idx_to_token[0] = self.pad_token\n","        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n","        self.vocab_len = len(self.idx_to_token)\n","\n","\n","class SurnamesDataset(Dataset):\n","    def __init__(self, surname, nat, vocab):\n","        self.surname = surname\n","        self.nat = nat\n","        self.vocab = vocab\n","\n","    def to_sequence(self, surname):\n","        tok_seq = [self.vocab.token_to_idx[tok] for tok in surname]\n","        padds_len = self.vocab.max_seq_len - len(tok_seq)\n","        pad_idx = self.vocab.token_to_idx[self.vocab.pad_token]\n","        padds = [pad_idx] * padds_len\n","        return torch.LongTensor(tok_seq + padds)\n","\n","    def __len__(self):\n","        return len(self.surname)\n","\n","    def __getitem__(self, idx):\n","        return self.to_sequence(self.surname.iloc[idx]), self.nat.iloc[idx]\n"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"yaSHtEtitnWC"},"outputs":[],"source":["vocab = Vocab(surname)\n","train_dataset = SurnamesDataset(x_train, y_train, vocab)\n","test_dataset = SurnamesDataset(x_test, y_test, vocab)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703019800268,"user":{"displayName":"Nudel","userId":"11250601100287929999"},"user_tz":-180},"id":"9Ijj4UtgvIx_"},"outputs":[],"source":["class RNNClassifierCustomRNN(nn.Module):\n","    def __init__(self, vocab_len, embedding_size, rnn_hidden_size, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_len, embedding_size, padding_idx=0)\n","        self.rnn = RNN(embedding_size, rnn_hidden_size)\n","        self.classifier = nn.Sequential(nn.Linear(rnn_hidden_size, rnn_hidden_size),\n","                                        nn.ReLU(),\n","                                        nn.Dropout(0.5),\n","                                        nn.Linear(rnn_hidden_size, num_classes))\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, surname):\n","        out = self.embedding(surname)  # batch x seq x emb\n","        out = self.rnn(out)[1]  # batch x rnn_hidden\n","        out = self.dropout(out)  # batch x rnn_hidden\n","        out = self.classifier(out)  # batch x num_classes\n","        return out\n"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cV08N0cO2wKX","executionInfo":{"status":"ok","timestamp":1703019821599,"user_tz":-180,"elapsed":21333,"user":{"displayName":"Nudel","userId":"11250601100287929999"}},"outputId":"d8c5703c-6a76-479b-e0e4-e9b8539be04d"},"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH:   1 \t LOSS: 2.5328 \t VAL LOSS: 2.3488\n","EPOCH:   3 \t LOSS: 2.1097 \t VAL LOSS: 2.0853\n","EPOCH:   5 \t LOSS: 1.9787 \t VAL LOSS: 2.0120\n","EPOCH:   7 \t LOSS: 1.7389 \t VAL LOSS: 1.7494\n","EPOCH:   9 \t LOSS: 1.6383 \t VAL LOSS: 1.6671\n","EPOCH:  11 \t LOSS: 1.5663 \t VAL LOSS: 1.5657\n","EPOCH:  13 \t LOSS: 1.5099 \t VAL LOSS: 1.5275\n","EPOCH:  15 \t LOSS: 1.4687 \t VAL LOSS: 1.5296\n","EPOCH:  17 \t LOSS: 1.4348 \t VAL LOSS: 1.4567\n","EPOCH:  19 \t LOSS: 1.4236 \t VAL LOSS: 1.4741\n"]}],"source":["model = RNNClassifierCustomRNN(\n","    vocab_len=vocab.vocab_len,\n","    embedding_size=128,\n","    rnn_hidden_size=64,\n","    num_classes=n_classes\n",")\n","\n","\n","def train(_model: torch.nn.Module, path: str = '/content/drive/MyDrive/surname_model.pt'):\n","    optimizer = torch.optim.Adam(_model.parameters(), weight_decay=0.001)\n","    loss = torch.nn.CrossEntropyLoss()\n","    loss_log = []\n","    loss_log_val = []\n","    min_val_loss = 10 ** 10\n","\n","    for i in range(20):\n","        epoch_loss = 0\n","        epoch_loss_val = 0\n","\n","        j, k = 1, 1  # Делители running losses\n","\n","        _model.train()\n","        for j, (batch_x, batch_y) in enumerate(train_loader):\n","            y_pred = _model(batch_x)\n","            running_loss = loss(y_pred, batch_y)\n","            epoch_loss += running_loss.item()\n","\n","            running_loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        _model.eval()\n","        for k, (batch_x, batch_y) in enumerate(test_loader):\n","            y_pred = _model(batch_x)\n","            running_loss = loss(y_pred, batch_y)\n","            epoch_loss_val += running_loss.item()\n","\n","        epoch_loss /= j\n","        epoch_loss_val /= k\n","\n","        if epoch_loss_val < min_val_loss:\n","            torch.save(_model.state_dict(), path)\n","\n","        if i % 2 == 0:\n","            print(f'EPOCH: {i + 1:3d} \\t LOSS: {epoch_loss:0.4f} \\t VAL LOSS: {epoch_loss_val:0.4f}')\n","\n","        loss_log.append(epoch_loss)\n","        loss_log_val.append(epoch_loss_val)\n","\n","    _model.load_state_dict(torch.load(path))\n","    return _model, loss_log, loss_log_val\n","\n","\n","model, loss_log, loss_log_val = train(model)"]},{"cell_type":"code","source":["!pip install kaleido"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5qonvZ-3Z0B","executionInfo":{"status":"ok","timestamp":1703020051651,"user_tz":-180,"elapsed":5224,"user":{"displayName":"Nudel","userId":"11250601100287929999"}},"outputId":"929327f8-7055-4728-e866-ec7679b797e0"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"]}]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"OsywI_kV2wKX","executionInfo":{"status":"error","timestamp":1703020051651,"user_tz":-180,"elapsed":6,"user":{"displayName":"Nudel","userId":"11250601100287929999"}},"outputId":"6e0bc587-5c21-49c6-c18d-61de9f7d5cb9"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-6580c8ce5af7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_log_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/basedatatypes.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3407\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_renderers.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Mimetype renderers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mbundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_mime_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderers_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mipython_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_renderers.py\u001b[0m in \u001b[0;36m_build_mime_bundle\u001b[0;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mbundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_mimebundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_base_renderers.py\u001b[0m in \u001b[0;36mto_mimebundle\u001b[0;34m(self, fig_dict)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_mimebundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         image_bytes = to_image(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Raise informative error message if Kaleido is not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \"\"\"\n\u001b[1;32m    135\u001b[0m \u001b[0mImage\u001b[0m \u001b[0mexport\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m\"kaleido\"\u001b[0m \u001b[0mengine\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkaleido\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"]}],"source":["px.line(pd.DataFrame({'train loss': loss_log, 'val loss': loss_log_val})).show(renderer='png', width=1000)"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9b2i_0Vp2wKY","executionInfo":{"status":"ok","timestamp":1703019908289,"user_tz":-180,"elapsed":327,"user":{"displayName":"Nudel","userId":"11250601100287929999"}},"outputId":"6bc68bc0-47c3-4054-9aa5-7a5167c00df2"},"outputs":[{"output_type":"stream","name":"stdout","text":["TEST ACCURACY: 0.6339\n"]}],"source":["right_answers = 0\n","for batch_x, batch_y in DataLoader(test_dataset, batch_size=32):\n","    predictions = model(batch_x).argmax(dim=1)\n","    right_answers += (torch.eq(batch_y, predictions)).sum()\n","\n","print(f'TEST ACCURACY: {right_answers / len(test_dataset):0.4f}')"]},{"cell_type":"markdown","metadata":{"id":"a2MIErKTo9aO"},"source":["|1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"id":"8vAFBul72wKY","executionInfo":{"status":"error","timestamp":1703019936337,"user_tz":-180,"elapsed":26083,"user":{"displayName":"Nudel","userId":"11250601100287929999"}},"outputId":"46529ff9-cb2a-49c8-f701-2700f7b7d44e"},"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH:   1 \t LOSS: 2.4830 \t VAL LOSS: 2.3603\n","EPOCH:   3 \t LOSS: 2.1539 \t VAL LOSS: 2.0810\n","EPOCH:   5 \t LOSS: 1.8729 \t VAL LOSS: 1.8147\n","EPOCH:   7 \t LOSS: 1.6855 \t VAL LOSS: 1.6578\n","EPOCH:   9 \t LOSS: 1.6143 \t VAL LOSS: 1.5981\n","EPOCH:  11 \t LOSS: 1.5367 \t VAL LOSS: 1.5546\n","EPOCH:  13 \t LOSS: 1.4952 \t VAL LOSS: 1.5246\n","EPOCH:  15 \t LOSS: 1.4530 \t VAL LOSS: 1.4665\n","EPOCH:  17 \t LOSS: 1.4120 \t VAL LOSS: 1.4259\n","EPOCH:  19 \t LOSS: 1.3971 \t VAL LOSS: 1.4445\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-ce303f763a4b>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_log_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_log_val_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_log_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_log_val_rnn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mright_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/basedatatypes.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3407\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_renderers.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Mimetype renderers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mbundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_mime_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderers_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mipython_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_renderers.py\u001b[0m in \u001b[0;36m_build_mime_bundle\u001b[0;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mbundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_mimebundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_base_renderers.py\u001b[0m in \u001b[0;36mto_mimebundle\u001b[0;34m(self, fig_dict)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_mimebundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         image_bytes = to_image(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Raise informative error message if Kaleido is not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \"\"\"\n\u001b[1;32m    135\u001b[0m \u001b[0mImage\u001b[0m \u001b[0mexport\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m\"kaleido\"\u001b[0m \u001b[0mengine\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkaleido\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"]}],"source":["class RNNClassifier(nn.Module):\n","    def __init__(self, vocab_len, embedding_size, rnn_hidden_size, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_len, embedding_size, padding_idx=0)\n","        self.rnn = torch.nn.RNN(embedding_size, rnn_hidden_size, batch_first=True)\n","        self.classifier = nn.Sequential(nn.Linear(rnn_hidden_size, rnn_hidden_size),\n","                                        nn.ReLU(),\n","                                        nn.Dropout(0.5),\n","                                        nn.Linear(rnn_hidden_size, num_classes))\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, surname):\n","        out = self.embedding(surname)  # batch x seq x emb\n","\n","        out = self.rnn(out)[1].squeeze(0)  # batch x rnn_hidden\n","\n","        out = self.dropout(out)  # batch x rnn_hidden\n","        out = self.classifier(out)  # batch x num_classes\n","        return out\n","\n","\n","model = RNNClassifier(\n","    vocab_len=vocab.vocab_len,\n","    embedding_size=128,\n","    rnn_hidden_size=64,\n","    num_classes=n_classes\n",")\n","\n","model, loss_log_rnn, loss_log_val_rnn = train(model)\n","px.line(pd.DataFrame({'train loss': loss_log_rnn, 'val loss': loss_log_val_rnn})).show(renderer='png', width=1000)\n","\n","right_answers = 0\n","for batch_x, batch_y in DataLoader(test_dataset, batch_size=32):\n","    predictions = model(batch_x).argmax(dim=1)\n","    right_answers += (torch.eq(batch_y, predictions)).sum()\n","\n","print(f'TEST ACCURACY: {right_answers / len(test_dataset):0.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbnzXY9b2wKY","executionInfo":{"status":"aborted","timestamp":1703019826128,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbK_hQ8m2wKY","executionInfo":{"status":"aborted","timestamp":1703019826128,"user_tz":-180,"elapsed":19,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_len, embedding_size, rnn_hidden_size, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_len, embedding_size, padding_idx=0)\n","        self.lstm = torch.nn.LSTM(embedding_size, rnn_hidden_size, batch_first=True)\n","        self.classifier = nn.Sequential(nn.Linear(rnn_hidden_size, rnn_hidden_size),\n","                                        nn.ReLU(),\n","                                        nn.Dropout(0.5),\n","                                        nn.Linear(rnn_hidden_size, num_classes))\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, surname):\n","        out = self.embedding(surname)  # batch x seq x emb\n","\n","        out = self.lstm(out)[1][0].squeeze(0)  # batch x rnn_hidden\n","\n","        out = self.dropout(out)  # batch x rnn_hidden\n","        out = self.classifier(out)  # batch x num_classes\n","        return out\n","\n","\n","model = LSTMClassifier(\n","    vocab_len=vocab.vocab_len,\n","    embedding_size=128,\n","    rnn_hidden_size=64,\n","    num_classes=n_classes\n",")\n","\n","model, loss_log_lstm, loss_log_val_lstm = train(model)\n","px.line(pd.DataFrame({'train loss': loss_log_lstm, 'val loss': loss_log_val_lstm})).show(renderer='png', width=1000)\n","\n","right_answers = 0\n","for batch_x, batch_y in DataLoader(test_dataset, batch_size=32):\n","    predictions = model(batch_x).argmax(dim=1)\n","    right_answers += (torch.eq(batch_y, predictions)).sum()\n","\n","print(f'TEST ACCURACY: {right_answers / len(test_dataset):0.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7q5bv5BT2wKY","executionInfo":{"status":"aborted","timestamp":1703019826128,"user_tz":-180,"elapsed":19,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["px.line(\n","    pd.DataFrame(\n","        {\n","            'train loss lstm': loss_log_lstm,\n","            'val loss lstm': loss_log_val_lstm,\n","            'train loss rnn': loss_log_rnn,\n","            'val loss rnn': loss_log_val_rnn,\n","            'train loss': loss_log,\n","            'val loss': loss_log_val\n","        }\n","    )\n",").show(renderer=None, width=1000)\n"]},{"cell_type":"markdown","metadata":{"id":"_6YBam_3t-fO"},"source":["1.3 Загрузите предобученные эмбеддинги (https://disk.yandex.ru/d/BHuT2tEXr_yBOQ?w=1) в модуль `nn.Embedding` и обучите модели из 1.2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OB8M7zX2wKY","executionInfo":{"status":"aborted","timestamp":1703019826129,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["with open('./models/globe_100_rows.txt') as file:\n","    weights = file.readlines()\n","    weights = list(map(str.split, weights))\n","    weights = {i[0]: torch.tensor(list(map(float, i[1:]))) for i in weights}\n","\n","model = LSTMClassifier(\n","    vocab_len=vocab.vocab_len,\n","    embedding_size=128,\n","    rnn_hidden_size=64,\n","    num_classes=n_classes\n",")\n","\n","for token, value in weights.items():\n","    try:\n","        model.embedding.weight[vocab.token_to_idx[token]] = value\n","    except Exception:\n","        pass\n","\n","model, loss_log_lstm, loss_log_val_lstm = train(model)\n","px.line(pd.DataFrame({'train loss': loss_log_lstm, 'val loss': loss_log_val_lstm})).show(renderer='png', width=1000)\n","\n","right_answers = 0\n","for batch_x, batch_y in DataLoader(test_dataset, batch_size=32):\n","    predictions = model(batch_x).argmax(dim=1)\n","    right_answers += (torch.eq(batch_y, predictions)).sum()\n","\n","print(f'TEST ACCURACY: {right_answers / len(test_dataset):0.4f}')"]},{"cell_type":"markdown","metadata":{"id":"f7kf990U9Do-"},"source":["## 2. Классификация обзоров на фильмы (RNN)\n","\n","Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n","\n","2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n","  * токен = __слово__\n","  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n","  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n","  * добавьте предобработку текста\n","\n","2.2. Обучите классификатор.\n","  \n","  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`\n","    - подберите адекватную размерность вектора эмбеддинга:\n","    - модуль `nn.Embedding` обучается\n","\n","  * Используйте рекуррентные слои (`nn.RNN`, `nn.LSTM`, `nn.GRU`)\n","\n","\n","2.3 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n","* Целевое значение accuracy на валидации - 70+%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHGZVLuG2wKY","executionInfo":{"status":"aborted","timestamp":1703019826129,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["# 2.0\n","\n","with open('./data/positive_reviews.txt') as file:\n","    pos_reviews = file.readlines()\n","\n","pos_reviews = pd.DataFrame(pos_reviews, columns=['review'])\n","pos_reviews['positive'] = 1\n","\n","with open('./data/negative_reviews.txt') as file:\n","    neg_reviews = file.readlines()\n","\n","neg_reviews = pd.DataFrame(neg_reviews, columns=['review'])\n","neg_reviews['positive'] = 0\n","\n","reviews = pd.concat([pos_reviews, neg_reviews])\n","\n","train_reviews, test_reviews = train_test_split(reviews, test_size=0.3)\n","\n","reviews.sample(5, random_state=63)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAqZZIsx2wKY","executionInfo":{"status":"aborted","timestamp":1703019826130,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["# 2.1\n","from nltk import WordNetLemmatizer\n","from nltk import word_tokenize\n","from tqdm import tqdm\n","\n","wnl = WordNetLemmatizer()\n","\n","\n","class Vocab:\n","    def __init__(self, data: pd.Series):\n","        _word_bag = {'<UNK>'}\n","        for sent in tqdm(data.review):\n","            sent = set(map(wnl.lemmatize, word_tokenize(sent)))\n","            _word_bag |= sent\n","\n","        self.idx_to_token = dict(zip(range(1, len(_word_bag) + 1), _word_bag))\n","\n","        self.token_to_idx = {v: u for u, v in self.idx_to_token.items()}\n","        self.vocab_len = len(self.idx_to_token)\n","        self._word_bag = _word_bag\n","\n","\n","vocab_review = Vocab(train_reviews)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9T79gRv2wKZ","executionInfo":{"status":"aborted","timestamp":1703019826130,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["class ReviewDataset(Dataset):\n","    def __init__(self, x: pd.Series, y: pd.Series, _vocab: Vocab):\n","        self.x = x\n","        self.y = y\n","        self.vocab = _vocab\n","\n","    def vectorize(self, review: str):\n","        MAX_REVIEW_LEN = 64\n","        review = review.lower()\n","        output_tensor = []\n","\n","        for l, word in enumerate(word_tokenize(review[:MAX_REVIEW_LEN]), 1):\n","\n","            word = wnl.lemmatize(word)\n","            if word in self.vocab.token_to_idx.keys():\n","                output_tensor.append(self.vocab.token_to_idx[word])\n","            else:\n","                output_tensor.append(self.vocab.token_to_idx['<UNK>'])\n","\n","        output_tensor = [0] * (MAX_REVIEW_LEN - len(output_tensor)) + output_tensor\n","\n","        return output_tensor\n","\n","    def __len__(self):\n","        return self.x.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = torch.LongTensor(self.vectorize(self.x.iloc[idx]))\n","        return x, torch.tensor(self.y.iloc[idx]).long()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnRQlhmK2wKZ","executionInfo":{"status":"aborted","timestamp":1703019826130,"user_tz":-180,"elapsed":19,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["\n","class RNNModel(torch.nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = torch.nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, (hidden, cell) = self.lstm(embedded)\n","        hidden = self.dropout(hidden.squeeze(0))\n","        out = self.fc(hidden)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exFsWBbE2wKZ","executionInfo":{"status":"aborted","timestamp":1703019826131,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["model = RNNModel(\n","    vocab_size=vocab_review.vocab_len + 1,\n","    embedding_dim=256,\n","    hidden_dim=128,\n","    output_dim=2,\n","    n_layers=2,\n","    dropout=0.5\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tu8U8iV82wKZ","executionInfo":{"status":"aborted","timestamp":1703019826131,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nudel","userId":"11250601100287929999"}}},"outputs":[],"source":["review_train_ds = ReviewDataset(train_reviews.review, train_reviews.positive, vocab_review)\n","review_valid_ds = ReviewDataset(test_reviews.review, test_reviews.positive, vocab_review)\n","\n","optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.01)\n","loss = torch.nn.CrossEntropyLoss()\n","loss_log = []\n","loss_log_val = []\n","min_val_loss = 10 ** 10\n","\n","for i in range(15):\n","    epoch_loss = 0\n","    epoch_loss_val = 0\n","    j, k = 1, 1  # Делители running losses\n","\n","    model.train()\n","    for j, (batch_x, batch_y) in enumerate(DataLoader(review_train_ds, batch_size=64, shuffle=True), 1):\n","        y_pred = model(batch_x)\n","        running_loss = loss(y_pred, batch_y)\n","        epoch_loss += running_loss.item()\n","\n","        running_loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    for k, (batch_x, batch_y) in enumerate(DataLoader(review_valid_ds, batch_size=64, shuffle=True), 1):\n","        y_pred = model(batch_x)\n","        running_loss = loss(y_pred, batch_y)\n","        epoch_loss_val += running_loss.item()\n","\n","    epoch_loss /= j\n","    epoch_loss_val /= k\n","\n","    if epoch_loss_val < min_val_loss:\n","        torch.save(model.state_dict(), 'models/review_model.pt')\n","\n","    if i % 1 == 0:\n","        print(f'EPOCH: {i + 1:3d} \\t LOSS: {epoch_loss:0.4f} \\t VAL LOSS: {epoch_loss_val:0.4f}')\n","\n","        right_answers = 0\n","        for batch_x, batch_y in DataLoader(review_valid_ds, batch_size=32):\n","            predictions = model(batch_x).argmax(dim=1)\n","            right_answers += (torch.eq(batch_y, predictions)).sum()\n","\n","        print(f'TEST ACCURACY: {right_answers / len(review_valid_ds):0.4f}')\n","\n","    loss_log.append(epoch_loss)\n","    loss_log_val.append(epoch_loss_val)\n","\n","model.eval()\n","model.load_state_dict(torch.load('models/review_model.pt'))"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}